{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Respond to natural language questions using RAG approach\n",
    "\n",
    "This notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation using a local model as well as watsonx.ai. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
    "\n",
    "#### About Retrieval Augmented Generation\n",
    "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
    "\n",
    "In its simplest form, RAG requires 3 steps:\n",
    "\n",
    "- Phase 1: Index knowledge base passages (once)\n",
    "- Phase 2: Retrieve relevant passage(s) from knowledge base (for every user query)\n",
    "- Phase 3: Generate a response by feeding retrieved passage into a large language model (for every user query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "##  Set up the environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Please restart the notebook kernel to pick up proper version of packages installed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Setup environment and import relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one of the main components will be a document file (we use a PDF) the main imports are pypdf to parse that and chromadb to set up the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "from enum import auto\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import chromadb\n",
    "\n",
    "from chromadb import Collection\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from pypdf import PdfReader, PageObject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper class/function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CollectionStatus(Enum):\n",
    "    COLLECTION_CREATED = auto()\n",
    "    COLLECTION_EXISTS = auto()\n",
    "\n",
    "\n",
    "def ensure_collection(client: chromadb.ClientAPI) -> tuple[CollectionStatus, Optional[Collection]]:\n",
    "    demo_collection = \"harry_potter\"\n",
    "    try:\n",
    "        client.get_collection(name=demo_collection)\n",
    "        return CollectionStatus.COLLECTION_EXISTS, None\n",
    "    except ValueError:\n",
    "        collection = client.get_or_create_collection(name=demo_collection, metadata={\"hnsw:space\": \"cosine\"})\n",
    "        return CollectionStatus.COLLECTION_CREATED, collection\n",
    "\n",
    "\n",
    "def clean_text(raw_text: str) -> str:\n",
    "    cleaned_text = raw_text.replace(\"\\n\", \" \")\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text) #space, tab or line break\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def get_chunks(pages: list[PageObject], max_words: int = 150) -> list[tuple[str, int]]:\n",
    "    text_tokens = [(clean_text(page.extract_text()).split(\" \"), page.page_number) for page in pages]\n",
    "    chunks = []\n",
    "\n",
    "    for idx, (words, page_number) in enumerate(text_tokens):\n",
    "        for i in range(0, len(words), max_words):\n",
    "            chunk = words[i:i + max_words]\n",
    "            if (i + max_words) > len(words) and (len(chunk) < max_words) and (\n",
    "                    len(text_tokens) != (idx + 1)):\n",
    "                next_page = text_tokens[idx + 1]\n",
    "                text_tokens[idx + 1] = (chunk + next_page[0], next_page[1])\n",
    "                continue\n",
    "            chunk = \" \".join(chunk).strip()\n",
    "            chunk = f'[Page no. {page_number}]' + ' ' + '\"' + chunk + '\"'\n",
    "            chunks.append((chunk, page_number))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def insert_document(document_path: Path, collection: Collection) -> None:\n",
    "    document_reader = PdfReader(document_path)\n",
    "    document_name = document_path.stem.replace(\" \", \"-\").replace(\"_\", \"-\")\n",
    "    pages = document_reader.pages\n",
    "\n",
    "    document_chunks = []\n",
    "    document_ids = []\n",
    "\n",
    "    chunks = get_chunks(pages)\n",
    "    for chunk_index, (chunk, page_number) in enumerate(chunks):\n",
    "        document_ids.append(f\"{document_name}_p{page_number}-{chunk_index}\")\n",
    "        document_chunks.append(chunk)\n",
    "\n",
    "    collection.add(\n",
    "        documents=document_chunks,\n",
    "        ids=document_ids,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Ingesting data & build up knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/Ingest_Data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_directory = Path(\"./\")\n",
    "db_directory = base_directory / \"db\"\n",
    "files_directory = base_directory / \"db_files\"\n",
    "\n",
    "if not db_directory.exists():\n",
    "    db_directory.mkdir()\n",
    "\n",
    "if not files_directory.exists():\n",
    "    print(\"DB files were not copied! Abort.\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=str(db_directory))\n",
    "\n",
    "collection_status, collection = ensure_collection(chroma_client)\n",
    "\n",
    "if collection_status == CollectionStatus.COLLECTION_EXISTS:\n",
    "    print(\"Collection already exists. No new files are loaded.\")\n",
    "else:\n",
    "    print(\"Creating collection...\")\n",
    "    for document_path in files_directory.glob(\"*.pdf\"):\n",
    "        insert_document(document_path, collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Optional: Check collection_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./db\")\n",
    "collection = client.get_collection(name=\"harry_potter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.peek(5)['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Excursus 1: Tokenization_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/Tokenization.png)\n",
    "\n",
    "Credits to Andreas, Hardy, Alex & Nils :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentence = [\"What did you eat for breaktfast?\"]\n",
    "\n",
    "tokens = model.tokenize(sentence)\n",
    "\n",
    "print(f\"Number of tokens: {len(tokens['input_ids'][0])}\")\n",
    "tokens['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Excursus 2: Embeddings_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/Embeddings.png)\n",
    "\n",
    "Credits to Andreas, Hardy, Alex & Nils :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentence)\n",
    "\n",
    "print(f\"Number of embeddings: {len(embeddings[0])}\")\n",
    "print(embeddings[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Retrieve relevant passage(s) from Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/Retrieve_Data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What was the job of Mr. Dursley?\" #Do Mr. and Mrs. Dursely have a son? #How old is Mr. Dursley?\n",
    "results = collection.query(\n",
    "    query_texts=[question],\n",
    "    n_results=2,\n",
    ")\n",
    "\n",
    "results['distances'], results['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Build prompt, pass to LLM & generate Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/Generate_Response.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, topn_chunks: list[str]):\n",
    "    prompt = \"Search results:\\n\"\n",
    "\n",
    "    for chunk in topn_chunks:\n",
    "        prompt += chunk + \"\\n\\n\"\n",
    "\n",
    "    prompt += \"Instructions: Compose a comprehensive reply to the query using the search results given. \" \\\n",
    "              \"If the search results mention multiple subjects \" \\\n",
    "              \"with the same name, create separate answers for each. Only include information found in the results and \" \\\n",
    "              \"don't add any additional information. Make sure the answer is correct and don't output false content. \" \\\n",
    "              \"If the text does not relate to the query, simply state 'Found Nothing'. Ignore outlier \" \\\n",
    "              \"search results which has nothing to do with the question. Only answer what is asked. The \" \\\n",
    "              \"answer should be short and concise.\"\n",
    "\n",
    "    prompt += f\"\\n\\n\\nQuery: {question}\\n\\nAnswer: \"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(question, results[\"documents\"][0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Interacting with the LLM on GPU (using watsonx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\", None) \n",
    "api_url = os.getenv(\"IBM_CLOUD_URL\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "\n",
    "creds = {\n",
    "    \"url\": api_url,\n",
    "    \"apikey\": api_key,\n",
    "\n",
    "}\n",
    "\n",
    "#creds, project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { \n",
    "    GenParams.DECODING_METHOD: \"sample\", \n",
    "    GenParams.MAX_NEW_TOKENS: 200, \n",
    "    GenParams.TEMPERATURE: 0.1\n",
    "    }\n",
    "\n",
    "model = Model(\"meta-llama/llama-2-70b-chat\", params=params, credentials=creds, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, response in enumerate(model.generate_text_stream(prompt)):\n",
    "    print(\"▌\") if count == 0 else print(response, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b Interacting with a small & quantized LLM on CPU (llama.cpp)\n",
    "\n",
    "The model deployed here is llama-2-7b-q8_0 (int8 quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "\n",
    "SERVER=\"127.0.0.1\" #server where you deployed llama.cpp, 127.0.0.1 => localhost\n",
    "PORT=\"8080\"\n",
    "\n",
    "json_data = {\n",
    "    'prompt': prompt,\n",
    "    'n_predict': 256,\n",
    "    'stream': True\n",
    "}\n",
    "\n",
    "client = httpx.AsyncClient()\n",
    "lastChunks = \"\"\n",
    "async with client.stream('POST', f'http://{SERVER}:{PORT}/completion', json=json_data) as response:\n",
    "    async for chunk in response.aiter_bytes():\n",
    "        try:\n",
    "            data = json.loads(chunk.decode('utf-8')[6:])\n",
    "        except:\n",
    "            pass\n",
    "        if data['stop'] is False:\n",
    "            print(data['content'], end=\"\")\n",
    "        else:\n",
    "            print('\\n\\n')\n",
    "            print(data['timings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further topics to consider\n",
    "\n",
    "- Performance Benchmarks?\n",
    "- Quantization (PTDQ, PTSQ, QAT)?\n",
    "- LangChain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
